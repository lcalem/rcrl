{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedExperienceReplay():\n",
    "    '''\n",
    "    alpha: tradeoff between sampling high priority transitions and random sampling\n",
    "    beta: used to compute importance sampling weights, increased from beta_start to 1.0 over the course of beta_steps\n",
    "    TL;DR of PER: https://medium.com/arxiv-bytes/summary-prioritized-experience-replay-e5f9257cef2d\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, max_size=500000, alpha=0.6, beta_start=0.4, beta_steps=100000):\n",
    "        self.max_size = max_size\n",
    "        self.replay_memory = []\n",
    "        self.priorities = np.zeros((max_size,), dtype=np.float32)\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.beta = beta_start\n",
    "        self.beta_incr = (1.0 - beta_start) / beta_steps\n",
    "        \n",
    "        self.index = 0\n",
    "        self.priorities[0] = 1.0**alpha  # init the first max prob\n",
    "        \n",
    "    def update_beta(self):\n",
    "        self.beta = min(1.0, self.beta + self.beta_incr)\n",
    "        \n",
    "    def get_probabilities(self):\n",
    "        '''\n",
    "        turn current priorities in probabilities\n",
    "        '''\n",
    "        size = len(self.replay_memory)\n",
    "        end_index = size if size < self.max_size else self.index\n",
    "        \n",
    "        prios = self.priorities[:end_index]\n",
    "        probs = prios / prios.sum()\n",
    "        \n",
    "    def insert(self, transition):\n",
    "        \n",
    "        # add the transition to the memory\n",
    "        if len(self.replay_memory) < self.max_size:\n",
    "            self.replay_memory.append(transition)\n",
    "        else:\n",
    "            self.replay_memory[self.index] = transition\n",
    "            \n",
    "        # update priorities and index\n",
    "        self.priorities[self.index] = self.priorities.max()\n",
    "        self.index = (self.index + 1) % self.max_size\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        '''\n",
    "        sample a batch of transitions\n",
    "        TODO: weigths\n",
    "        '''\n",
    "        current_size = len(replay_buffer)\n",
    "        \n",
    "        # samples\n",
    "        probs = self.get_probabilities()\n",
    "        indices = np.random.choice(current_size, batch_size, p=probs)\n",
    "        samples = [self.replay_memory[i] for i in indices]\n",
    "        #samples = random.sample(self.replay_memory, batch_size)\n",
    "        \n",
    "        # importance sampling weights\n",
    "        prob_min = probs.min()\n",
    "        max_weight = (prob_min * total)**(-beta)\n",
    "\n",
    "        is_weights  = (current_size * probs[indices]) ** (-self.beta)\n",
    "        is_weights /= max_weight  # to ensure it is not > 1\n",
    "        \n",
    "        self.update_beta()\n",
    "        return samples, is_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main DQN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(sess,\n",
    "        env):\n",
    "    \n",
    "    replay_memory = PrioritizedExperienceReplay()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "exp_folder = \"experiments/exp%s\" % (datetime.datetime.now().strftime(\"%Y%m%d%H%M\"))\n",
    "\n",
    "# main run\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in dqn(sess,\n",
    "                        env):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
